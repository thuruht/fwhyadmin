#!/usr/bin/env node

/**
 * Focused Blog Migration Script
 * 
 * Migrates blog posts from bl0wd1 D1 database to unified backend
 * Targets the specific table structure we know exists
 */

const { execSync } = require('child_process');

// Target database and destination
const BLOG_DB = 'bl0wd1';
const NEW_BLOG_KV = '6ee9ab6b71634a4eb3e66de82d8dfcdc'; // unified backend blog KV

console.log('üöÄ Starting focused blog migration from bl0wd1...\n');

async function runCommand(command) {
  try {
    console.log(`Executing: ${command}`);
    const result = execSync(command, {
      encoding: 'utf8',
      maxBuffer: 50 * 1024 * 1024, // 50MB buffer for large data
      stdio: ['pipe', 'pipe', 'pipe']
    });
    return result;
  } catch (error) {
    console.error(`Error executing command: ${command}`);
    console.error(`Error message:`, error.message);
    return null;
  }
}

async function migrateBlogPosts() {
  console.log('üìù Migrating blog posts from bl0wd1 database...');
  
  try {
    // First, get the table structure to see what we're working with
    console.log('üîç Checking table structure...');
    const tablesResult = await runCommand(`wrangler d1 execute ${BLOG_DB} --remote --json --command "SELECT name FROM sqlite_master WHERE type='table';"`);
    
    if (!tablesResult) {
      console.error('‚ùå Could not fetch table list');
      return;
    }
    
    let tables = [];
    try {
      const tablesData = JSON.parse(tablesResult);
      tables = (tablesData.results || []).map(t => t.name);
    } catch (e) {
      console.log('‚ö†Ô∏è  JSON parse failed, trying direct table access...');
      // Fallback: we know blog_posts exists
      tables = ['blog_posts'];
    }
    
    console.log('üìã Available tables:', tables.join(', '));
    
    // Look for the blog_posts table specifically (we know it exists)
    const blogTable = tables.find(t => 
      t.toLowerCase() === 'blog_posts' ||
      t.toLowerCase().includes('blog') || 
      t.toLowerCase().includes('post') || 
      t.toLowerCase().includes('article')
    ) || 'blog_posts'; // default to blog_posts since we know it exists
    
    if (!blogTable || blogTable === '') {
      console.log('‚ùå No suitable blog table found');
      return;
    }
    
    console.log(`‚úÖ Using table: ${blogTable}`);
    
    // Get column structure
    console.log('üîç Checking column structure...');
    const schemaResult = await runCommand(`wrangler d1 execute ${BLOG_DB} --remote --json --command "PRAGMA table_info(${blogTable});"`);
    
    if (schemaResult) {
      const schemaData = JSON.parse(schemaResult);
      const columns = (schemaData.results || []).map(c => c.name);
      console.log('üìã Available columns:', columns.join(', '));
    }
    
    // Count total posts
    console.log('üî¢ Counting total posts...');
    const countResult = await runCommand(`wrangler d1 execute ${BLOG_DB} --remote --json --command "SELECT COUNT(*) as count FROM ${blogTable};"`);
    
    if (!countResult) {
      console.error('‚ùå Could not count posts');
      return;
    }
    
    let totalPosts = 0;
    try {
      const countData = JSON.parse(countResult);
      totalPosts = countData.results?.[0]?.count || 0;
    } catch (e) {
      console.log('‚ö†Ô∏è  JSON parse failed for count, trying direct approach...');
      // We know there are 5 posts from our manual check
      totalPosts = 5;
    }
    console.log(`üìä Found ${totalPosts} total posts to migrate`);
    
    if (totalPosts === 0) {
      console.log('‚úÖ No posts to migrate');
      return;
    }
    
    // Migrate posts in batches without processing the content
    const BATCH_SIZE = 10; // Small batches to avoid overwhelming
    let migratedCount = 0;
    
    for (let offset = 0; offset < totalPosts; offset += BATCH_SIZE) {
      console.log(`üîÑ Processing batch ${Math.floor(offset/BATCH_SIZE) + 1}/${Math.ceil(totalPosts/BATCH_SIZE)} (posts ${offset + 1}-${Math.min(offset + BATCH_SIZE, totalPosts)})`);
      
      // Fetch batch
      const batchResult = await runCommand(`wrangler d1 execute ${BLOG_DB} --remote --json --command "SELECT * FROM ${blogTable} ORDER BY rowid LIMIT ${BATCH_SIZE} OFFSET ${offset};"`);
      
      if (!batchResult) {
        console.error(`‚ùå Failed to fetch batch at offset ${offset}`);
        continue;
      }
      
      // Parse and migrate without processing content
      try {
        const batchData = JSON.parse(batchResult);
        const posts = batchData.results || [];
        
        for (const post of posts) {
          // Create a migration-friendly key
          const postId = post.id || post.rowid || `post_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
          const migrationKey = `blog_${postId}`;
          
          // Store post data directly to KV without processing
          const kvCommand = `wrangler kv key put "${migrationKey}" '${JSON.stringify(post).replace(/'/g, "\\'")}' --namespace-id ${NEW_BLOG_KV}`;
          const kvResult = await runCommand(kvCommand);
          
          if (kvResult !== null) {
            migratedCount++;
            console.log(`  ‚úÖ Migrated post: ${migrationKey}`);
          } else {
            console.log(`  ‚ùå Failed to migrate post: ${migrationKey}`);
          }
        }
        
      } catch (parseError) {
        console.error(`‚ùå Failed to parse batch at offset ${offset}:`, parseError.message);
      }
    }
    
    console.log(`\nüéâ Migration complete! Successfully migrated ${migratedCount}/${totalPosts} posts`);
    
  } catch (error) {
    console.error('‚ùå Migration failed:', error.message);
  }
}

// Run migration
migrateBlogPosts()
  .then(() => {
    console.log('\n‚úÖ Blog migration script completed');
  })
  .catch(error => {
    console.error('\n‚ùå Script failed:', error.message);
    process.exit(1);
  });
